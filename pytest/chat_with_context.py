# import os, copy, types, gc, sys, re  # å¯¼å…¥æ“ä½œç³»ç»Ÿã€å¯¹è±¡å¤åˆ¶ã€ç±»å‹ã€åƒåœ¾å›æ”¶ã€ç³»ç»Ÿã€æ­£åˆ™è¡¨è¾¾å¼ç­‰åŒ…
# import torch # å¯¼å…¥ pytorch åº“
# import numpy as np  # å¯¼å…¥ numpy åº“
# os.environ['RWKV_JIT_ON'] = '0'
# os.environ['RWKV_CUDA_ON'] = '0'

# from rwkv.model import RWKV
# from rwkv.utils import PIPELINE

# print(" åŠ è½½æ¨¡å‹ä¸­...")
# model = RWKV(model='/workspace/model/rwkv7-g1a4-2.9b-20251118-ctx8192', strategy='cuda fp16')
# pipeline = PIPELINE(model, "rwkv_vocab_v20230424")

# print(" æ¨¡å‹åŠ è½½å®Œæˆï¼")
# print(" æ”¯æŒå¤šè½®å¯¹è¯ï¼Œä¼šè®°ä½ä¸Šä¸‹æ–‡")
# print(" è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
# print(" è¾“å…¥ 'exit' é€€å‡º\n")

# # å¯¹è¯å†å²
# conversation_history = []

# while True:

#     user_input = input("ä½ : ")
    
#     if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
#         print(" å†è§ï¼")
#         break
    
#     if user_input.lower() in ['clear', 'æ¸…ç©º']:
#         conversation_history = []
#         print("  å¯¹è¯å†å²å·²æ¸…ç©º\n")
#         continue
    
#     if not user_input.strip():
#         continue
    
#     # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
#     conversation_history.append(f"User: {user_input}")
#     # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
#     # context = "\n".join(conversation_history) + "\nAssistant:<think"
#     context = "\n".join(f"User: {user_input}") + "\nAssistant:<think"
    
#     # ç”Ÿæˆå›å¤
#     print(" æ€è€ƒä¸­...", end='\r')
#     response = pipeline.generate(context, token_count=400)
    
#     # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤å¯èƒ½çš„å‰ç¼€ï¼‰
#     response = response.strip()
#     if response.startswith("Assistant:<think"):
#         response = response[10:].strip()
    
#     # æ·»åŠ å›å¤åˆ°å†å²
#     conversation_history.append(f"Assistant:<think {response}")
    
#     print(f"AI: {response}\n")
    
#     # é™åˆ¶å†å²é•¿åº¦ï¼ˆé˜²æ­¢è¶…å‡ºä¸Šä¸‹æ–‡çª—å£ï¼‰
#     if len(conversation_history) > 20:
#         conversation_history = conversation_history[-20:]

import os, copy, types, gc, sys, re  # å¯¼å…¥æ“ä½œç³»ç»Ÿã€å¯¹è±¡å¤åˆ¶ã€ç±»å‹ã€åƒåœ¾å›æ”¶ã€ç³»ç»Ÿã€æ­£åˆ™è¡¨è¾¾å¼ç­‰åŒ…
import torch # å¯¼å…¥ pytorch åº“
import numpy as np  # å¯¼å…¥ numpy åº“
os.environ['RWKV_JIT_ON'] = '0'
os.environ['RWKV_CUDA_ON'] = '0'


from rwkv.model import RWKV
from rwkv.utils import PIPELINE

class ChatBot:
    def __init__(self, model_path):
        print("ğŸ¤– åŠ è½½æ¨¡å‹ä¸­...")
        self.model = RWKV(model=model_path, strategy='cuda fp16')
        self.pipeline = PIPELINE(self.model, "rwkv_vocab_v20230424")
        self.conversation = []
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    def generate_response(self, prompt, max_tokens=400):
        """æ‰‹åŠ¨ç”Ÿæˆï¼Œåªè¿”å›æ–°å†…å®¹"""
        # ç¼–ç è¾“å…¥
        tokens = self.pipeline.encode(prompt)
        state = None
        
        # å¤„ç†è¾“å…¥ tokensï¼ˆä¸è¾“å‡ºï¼‰
        for token in tokens:
            _, state = self.model.forward([token], state)
        
        # ç”Ÿæˆæ–°å†…å®¹
        output_tokens = []
        generated_text = ""
        
        print("AI: ", end='', flush=True)
        
        for i in range(max_tokens):
            # å‰å‘ä¼ æ’­
            if i == 0 and len(tokens) > 0:
                out, state = self.model.forward([tokens[-1]], state)
            else:
                out, state = self.model.forward([output_tokens[-1]], state)
            
            # é‡‡æ ·ï¼ˆåªä½¿ç”¨å…¼å®¹çš„å‚æ•°ï¼‰
            token = self.pipeline.sample_logits(
                out, 
                temperature=1.0,  # æ§åˆ¶éšæœºæ€§ï¼š0.8 æ›´ä¿å®ˆï¼Œ1.2 æ›´éšæœº
                top_p=0.7,        # æ ¸é‡‡æ ·ï¼š0.7-0.9 éƒ½ä¸é”™
                top_k=0           # 0 è¡¨ç¤ºä¸ä½¿ç”¨ top_k
            )
            
            # æ£€æŸ¥ç»“æŸç¬¦
            if token == 0:
                break
            
            output_tokens.append(token)
            
            # å®æ—¶è§£ç å¹¶è¾“å‡º
            tmp = self.pipeline.decode(output_tokens)
            if '\ufffd' not in tmp:
                new_part = tmp[len(generated_text):]
                print(new_part, end='', flush=True)
                generated_text = tmp
            
            # æ£€æµ‹åœæ­¢æ ‡è®°
            stop_marks = ['\nUser:', '\nä½ :', '\nHuman:', 'User:', 'ä½ :']
            if any(mark in generated_text for mark in stop_marks):
                for mark in stop_marks:
                    if mark in generated_text:
                        generated_text = generated_text.split(mark)[0]
                        break
                break
        
        print("\n")
        return generated_text.strip()
    
    def chat(self, user_input):
        """å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤"""
        # 1. æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²
        self.conversation.append(f"User: {user_input}")
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æœ€è¿‘ 10 è½®å¯¹è¯ï¼‰
        recent = self.conversation[-20:]
        context = "\n".join(recent) + "\nAssistant: <think>"
        
        # 3. ç”Ÿæˆå›å¤ï¼ˆåªè·å–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = self.generate_response(context, max_tokens=800)
        
        # 4. æ¸…ç†å›å¤
        response = response.replace("</think>", "").strip()
        if response.startswith("<think>"):
            response = response[7:].strip()
        
        # 5. ä¿å­˜åˆ°å†å²
        self.conversation.append(f"Assistant: <think> {response}")
        
        # 6. é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation) > 40:
            self.conversation = self.conversation[-40:]
        
        return response
    
    def clear(self):
        self.conversation = []
        print("ğŸ—‘ï¸  å¯¹è¯å†å²å·²æ¸…ç©º\n")
    
    def show_history(self):
        print("\n" + "="*60)
        print("ğŸ“ å¯¹è¯å†å² (å…± {} æ¡):".format(len(self.conversation)))
        print("="*60)
        for i, line in enumerate(self.conversation, 1):
            print(f"{i}. {line}")
        print("="*60 + "\n")


def main():
    model_path = '/workspace/model/rwkv7-g1a4-2.9b-20251118-ctx8192'
    chatbot = ChatBot(model_path)
    
    print("ğŸ’¬ RWKV äº¤äº’å¼å¯¹è¯")
    print("="*60)
    print("å‘½ä»¤:")
    print("  exit/quit    - é€€å‡º")
    print("  clear        - æ¸…ç©ºå†å²")
    print("  history      - æŸ¥çœ‹å†å²")
    print("="*60)
    print()
    
    while True:
        try:
            user_input = input("ä½ : ")
            
            if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() in ['clear', 'æ¸…ç©º']:
                chatbot.clear()
                continue
            
            if user_input.lower() in ['history', 'å†å²']:
                chatbot.show_history()
                continue
            
            if not user_input.strip():
                continue
            
            # ç”Ÿæˆå›å¤
            chatbot.chat(user_input)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}\n")


if __name__ == "__main__":
    main()